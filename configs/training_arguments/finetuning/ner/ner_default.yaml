expected_batch_size: 32
training_arguments:
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 1
  learning_rate: 2.0e-5
  num_train_epochs: 5
  evaluation_strategy: 'epoch'
  logging_steps: 25
  save_steps: 40000