training_arguments:
  per_device_train_batch_size : 32
  num_train_epochs: 5
  do_eval: True
  logging_steps: 25
  eval_steps: 2500
  save_steps: 2500
  learning_rate: 2.0e-5
  evaluation_strategy: 'steps'
  disable_tqdm: False
  gradient_accumulation_steps: 1
  metric_for_best_model: 'eval_es_accuracy'
  greater_is_better: True
  load_best_model_at_end: True
  save_total_limit: 3
other_arguments:
  early_stopping_patience: 15
